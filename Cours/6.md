---
tags:
  - AI
  - Brain
  - MachineLearning
  - Neuroscience
  - Mind
Source: "[[Cours Cognitive, behavioral and social data]]"
Date: 2025-02-02
---
#### Definition

```ad-abstract
title:Definition
- **Brain-reading** : analysis of the activation of multiple voxels in the brain evoked by a stimulus, detected by fMRI, in order to decode the original stimulus.
```

```ad-example
```

![[Brain reading example 1 - image.png]]

#### Type of pattern analysis

```ad-success
title:Univariate analysis
- **Each voxel is treated as an independent piece of data**, using statistical tests to determine whether that voxel responded more in some experimental conditions than others
- The analysis of one voxel has no impact on the analysis of any other
```

![[Univariate analysis - image.png]]

```ad-info
title:Multivoxel pattern analysis
- This analysis is designed to **test whether two (or more) experimental conditions** can be distinguished from one another based on the activity patterns observed in a set of voxels
- So, it extracts the information contained in the patterns of activity among multiple voxels
- Multivariate methods are able to tell apart the activity patterns for two different conditions, even if the average level of activity does not differ between conditions
```

![[MVPA -image.png]]

#### fMRI data for Machine Learning

**Learning problem** :  to train classifiers to distinguish cognitive states at a single time instant or interval

**Classification function** :
$$f : \text{fMRI-sequence}(t_1, t_2) → \text{CognitiveState}$$
where : 
- fMRI-sequence($t_1, t_2$) is the sequence of fMRI images collected during the contiguous time interval \[t1, t2\],
- CognitiveState is the set of cognitive states to be discriminated.

```ad-info
```

It is often **advantageous to reduce the number of features considered** to focus on a subset of particular interest.
This is called [[Feature selection]].

Methods to reduce the number of features are:
- To select the voxels in a particular region of interest (ROI) or their values at a particular point in a trial
- Dimensionality reduction : 
	- [[Principal Component Analysis (PCA)]]
	- Independent Components Analysis (ICA)

```ad-question
title:How to train the algorithm ?
```

- ~~Splitting the dataset in half~~
- **leave-one-out [[Cross-validation]] :
	- leave one example out, train on the remaining ones, make a prediction for this example
	- repeat for each example in turn
	- compute the accuracy of the predictions made for all the examples
- **k-fold cross-validation** : where k is the number of parts in which the dataset is divided; common choices are k = 10 or k = 5, corresponding to leaving out 10% or 20% of the examples on each fold.

```ad-note
```

- The most used measure of how well a classifier does on the test set is its **accuracy** : fraction of examples in the test set for which the correct label was predicted

```ad-example
title:Examples
```

- **Goal** : to test whether a classifier could distinguish the activation as a result of seeing words that were either kinds of tool or kinds of building.
- **Task** : the subject was shown one word per trial; he should think about the item and its properties while the word was displayed (3 seconds) and try to clear her mind afterwards (8 seconds of blank screen).

![[example fMRI and dat - image 1.png]]

**Experiment structure** :
- 7 different items belonging to each of the two categories = 14 items
- experimental procedure repeated for 6 times (6 experimental epochs)
- Images were acquired with a time resolution of 1 second
- Voxel dimensions 3×3×5 mm

**Features selection** :
- A brain mask was extracted automatically and used to restrict voxels considered in the subsequent steps to roughly 16000 (definition of a Region of Interest, ROI)

**Conversion of raw data into a set of examples** :
Each trial was converted into an example by taking the average image during a 4 second span while the subject was thinking about the stimulus shown a few seconds earlier; these 4 seconds contained the expected peak of the signal during the trial, based on the standard BOLD hemodynamic response function.

Each example was thus a vector containing the 16000 feature values, corresponding to the 4 second average signal at each of the voxels.

Finally, we have a total of 84 examples = 7 items x 2 classes (tools and buildings) × 6 experimental repetitions.

![[example fMRI and ML 2 - image.png]]

**Splitting data in training and test sets** :
The dataset for one subject was split in two dataset. For example, the data from the experiment epochs 1,3 and 5 could be the training set (42 examples) and those in 2,4 and 6 could be the test set (42 examples).

![[example fMRI and ML 3 - image.png]]

**Run a linear classifier using a k-fold cross-validation** :
For example, split the dataset in 6 folds, each one corresponds to an experimental epoch

![[example fMRI and ML 4 - image.png]]

**Run a linear classifier** :
From the point of view of a geometric intuition, learning a linear classifier is equivalent to learning a line that separates points in the two classes as well as possible (decision boundary).

![[example fMRI and ML 5 - image.png]]

**Interpreting results** :
The “true accuracy” is the probability that the classifier will correctly label a new example

#### From brain imaging to brain reading

```ad-info
title:First study
```

Pioneering study by Haxby and colleagues (2001) : 
They demonstrated that **activity patterns in ventral temporal cortex can accurately discriminate between multiple object categories**

The authors used fMRI to measure patterns of BOLD activity, focusing on object-responsive regions in the ventral temporal cortex.

By comparing the similarity of brain activity patterns between the first and second half of the experiment, the authors showed that these high-level object areas could accurately predict whether participants were viewing pictures of faces, houses, chairs, cats, bottles, shoes, scissors, or scrambled stimuli

![[1st study brain reading - image.png]]

```ad-info
title:Next studies...
```

The use of more sophisticated pattern classification algorithms greatly improved researchers’ ability to predict what object category people were viewing.

In 2008, Gallant’s team developed a **decoder that could identify which of 120 pictures a subject was viewing** — a much bigger challenge than inferring what general category an image belongs to.

The **next step was reconstructing the image according to the brain activity pattern**. 
An early success at reconstructing fragments of simple shapes was obtained by Miyawaki et al 2008.

```ad-success
title:Reconstructing image from brain activity
```

In a fMRI study, observers were presented with hundreds of different random patterns of flickering checks placed within a 10×10 square grid, and pattern analysis was used to predict whether any given tile of the grid was flickering or not. Using this model, the authors could effectively reconstruct novel stimuli shown to the participant, including simple shapes and letters (Figure a).

Moreover, the authors could reconstruct the viewed stimulus from single brain volumes to show how this information evolved over the time course of the BOLD response (Figure b).

![[Reconstructing image from brain activity - image.png]]

See : https://www.youtube.com/watch?v=daY7uO0eftA&t=4s

```ad-question
title:So far, brain reading or mind reading ?
```

```ad-abstract
title:Brain reading
- Prediction about the viewed stimulus
- The experimenter does not need a mind-reading device to achieve this performance
- One could perform these same feats by reading out the activity patterns formed on the retina, even though conscious processing of the image has yet to take place. Activity pattern on the retina would remain robust even if the person were anaesthetized or fell into a deep coma
```

```ad-info
title:Mind-reading
- Information being decoded from the person’s brain is fundamentally private and subjective
- The only alternative would be to ask the participant directly about what she is thinking and to hope for an honest reply.
- Ongoing research is just beginning to probe the possibilities and limits of reading out subjective information from the human brain
```


![[brain reading example - image.png]]

```ad-seealso
title:Detecting Awareness in the Vegetative State
```

**Searching for evidence of consciousness in patients in vegetative state** :
- not really machine learning, more statistics, but still interesting
- patients asked to perform imaginary tasks
- playing tennis : activity in supplementary motor area
- walking through own home: parahippocampal gyrus, the posterior parietal cortex, and the lateral premotor cortex

**Sign of consciousness ?**
- The patient collaborated with experimenters imaging what she was told to?
- Or just an automatic response?
- Mere coincidence?


#### See also

- https://youtu.be/zVAVGKFw5NU
- Miyawaki Y, Uchida H, Yamashita O, Sato MA, Morito Y, et al. 2008 Visual image reconstruction from human brain activity using a combination of multiscale local image decoders. Neuron 60:915–29.
- Shinji Nishimoto, An T. Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, Jack L. Gallant (2011). Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies. Current Biology, 21(19): 1641–1646.
- Adrian M. Owen, Martin R. Coleman, Melanie Boly, Matthew H. Davis, Steven Laureys, John D. Pickard (2006). Detecting Awareness in the Vegetative State. Science, 313(5792):1402.
- Tang, J., LeBel, A., Jain, S. et al. Semantic reconstruction of continuous language from non-invasive brain recordings. Nat Neurosci 26, 858–866 (2023).
